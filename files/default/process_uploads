#!/usr/bin/env ruby

gem 'aws-sdk', '~> 1.0'
require 'aws-sdk'
require 'find'
require 'shellwords'
require 'fileutils'
require 'net/http'
require 'net/http/post/multipart'
require 'uri'
require 'json'
require 'zlib'
require 'zip'
require 'date'
require 'yaml'

def conf
  @conf ||= YAML.load_file('/opt/evertrue/config.yml')
end

def auth_oid_query_string(oid)
  @auth_oid_query_string ||= "?oid=#{oid}&auth=#{conf[:upload_auth_token]}&auth_provider=evertrueapptoken&app_key=#{conf[:upload_app_key]}"
end

def auth_query_string
  @auth_query_string ||= "?auth=#{conf[:upload_auth_token]}&auth_provider=evertrueapptoken&app_key=#{conf[:upload_app_key]}"
end

def get_from_api(uri)
  uri = URI.parse(URI.encode(uri))

  http = Net::HTTP.new(uri.host, uri.port)
  http.use_ssl = true

  req = Net::HTTP::Get.new(uri.request_uri)
  response = http.request(req)
  fail "API error, Response: #{response.code}, body: #{response.body}" unless response.code.to_i == 200

  JSON.parse(response.body)
end

def post_to_api(uri, data)
  uri = URI.parse(URI.encode(uri))
  http = Net::HTTP.new(uri.host, uri.port)
  http.use_ssl = true

  req = Net::HTTP::Post.new(uri.request_uri)
  unless data.empty?
    req['Content-Type'] = 'application/json'
    req.body = data.to_json
  end

  response = http.request(req)
  fail "Error posting to API, data: #{data.inspect}. Response: #{response.code}, body: #{response.body}" unless response.code.to_i == 200

  JSON.parse(response.body)
end

def get_dna(org_slug, key)
  get_from_api(conf[:api_url] + "/1.0/#{org_slug}/dna/#{key}")['response']['data']
rescue Exception => e
  puts "Error sending org_slug: #{org_slug}"
  puts e.message
  puts e.backtrace
end

def send_to_s3(org_slug, path)
  s3 = AWS::S3.new(
    access_key_id: conf[:aws_access_key_id],
    secret_access_key: conf[:aws_secret_access_key]
  )
  bucket = s3.buckets['onboarding.evertrue.com']
  now = DateTime.now.strftime('%Q')
  s3_filename = "#{now}-#{File.basename(path)}"
  bucket.objects["#{org_slug}/data/#{s3_filename}"].write(Pathname.new(path))
  s3_filename
end

def get_oid(org_slug)
  get_from_api(conf[:api_url] + "/auth/organizations/slug/#{org_slug}" + auth_query_string)['id']
rescue Exception => e
  puts "Error sending org_slug: #{org_slug}"
  puts e.message
  puts e.backtrace
end

def send_to_new_importer(org_slug, oid, s3_filename, compression, is_full_import, is_gift_import, auto_ingest)
  job_id = post_to_new_importer(oid, s3_filename, compression, is_full_import, is_gift_import)

  if auto_ingest == 0
    puts "skipped auto-ingestion for #{org_slug}"
    return
  end

  queue_to_new_importer(oid, job_id)
end

def get_header_change_status_from_importer(oid, s3_filename, compression, is_full_import, is_gift_import)
  data = { 's3_filename' => s3_filename, 'notify' => 1, 'compression' => compression }
  data['type'] = 'TRANSACTIONAL_CSV' if is_gift_import

  get_from_api(conf[:api_url] + '/importer/v1/jobs' + auth_oid_query_string(oid), data)['mappingExists']
rescue Exception => e
  puts "Error sending oid: #{oid}, s3_filename: #{s3_filename}"
  puts e.message
  puts e.backtrace
end

def post_to_new_importer(oid, s3_filename, compression, is_full_import, is_gift_import)
  data = { 's3_filename' => s3_filename, 'notify' => 1, 'compression' => compression }

  if is_gift_import
    data['type'] = 'TRANSACTIONAL_CSV'
  else
    data['prune'] = is_full_import
  end

  post_to_api(conf[:api_url] + '/importer/v1/jobs' + auth_oid_query_string(oid), data)['id']
rescue Exception => e
  puts "Error sending oid: #{oid}, s3_filename: #{s3_filename}"
  puts e.message
  puts e.backtrace
end

def queue_to_new_importer(oid, job_id)
  post_to_api(conf[:api_url] + "/importer/v1/jobs/queue/#{job_id}" + auth_oid_query_string(oid), {})
rescue Exception => e
  puts "Error queueing oid: #{oid}, job_id: #{job_id}"
  puts e.message
  puts e.backtrace
end

def compress_if_not_already(path, compression)
  return path if compression != 'NONE'

  Zlib::GzipWriter.open("#{path}.gz", Zlib::BEST_COMPRESSION) do |gz|
    gz.mtime = File.mtime(path)
    gz.orig_name = File.basename(path)
    File.open(path) do |f|
      IO.copy_stream(f, gz)
    end
  end

  FileUtils.rm(path)
  "#{path}.gz"
end

def process(org_slug, path, compression, auto_ingest)
  return unless `lsof #{Shellwords.shellescape path}`.empty?

  is_full_import = !(File.basename(path) =~ /\.full\./i).nil?
  is_gift_import = !(File.basename(path) =~ /\.gifts\./i).nil?

  oid = get_oid(org_slug)
  s3_filename = send_to_s3(org_slug, path)

  mappingExists = get_header_change_status_from_importer(oid, s3_filename, compression, is_full_import, is_gift_import)

  if mappingExists
    send_to_new_importer(org_slug, oid, s3_filename, compression, is_full_import, is_gift_import, auto_ingest)
    puts "sent file #{path} for processing"

    compressed_path = compress_if_not_already(path, compression)

    FileUtils.chmod(0700, compressed_path)
    FileUtils.chown('root', 'root', compressed_path)
    FileUtils.mv(compressed_path, '/var/evertrue/uploads')
  else
    puts "Error processing #{path} for #{org_slug}: CSV headers have changed since last import"
  end
end

def main
  processed_users = conf[:unames].each_with_object([]) do |uname, m|
    next if uname == 'trial0928'
    org_slug = /(.*?)\d+$/.match(uname)[1]

    begin
      auto_ingest = get_dna(org_slug, 'ET.Importer.IngestionMode')
      if auto_ingest.nil? || auto_ingest != 'AutoIngest'
        auto_ingest = 0
      else
        auto_ingest = 1
      end

      Find.find("/home/#{uname}/uploads") do |path|
        begin
          case path
          when /.*\.csv$/i
            process(org_slug, path, 'NONE', auto_ingest)
            m << uname
          when /.*\.gz$/i
            process(org_slug, path, 'GZIP', auto_ingest)
            m << uname
          when /.*\.zip$/i
            process(org_slug, path, 'ZIP', auto_ingest)
            m << uname
          end
        rescue => e
          puts "Error processing #{path}: #{e}"
        end
      end
    rescue => e
      puts "Error processing #{org_slug}: #{e}"
    end
  end

  puts "Uploaded data from: #{processed_users.join(', ')}" unless processed_users.empty?
end

main
