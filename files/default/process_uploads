#!/usr/bin/env ruby2.0

# Generated by Chef for <%= node['fqdn'] %>
# Local modifications will be overwritten.

gem 'aws-sdk', '~> 1.0'
require 'aws-sdk'
require 'shellwords'
require 'net/http'
require 'net/http/post/multipart'
require 'uri'
require 'json'
require 'zlib'
require 'zip'
require 'date'
require 'yaml'
require 'pagerduty'
require 'sentry-raven'
require 'logger'
require 'pony'
require 'trollop'

def opts
  @opts ||= Trollop.options { opt :debug, 'Debug mode', short: '-d' }
end

def conf
  @conf ||= YAML.load_file('/opt/evertrue/config.yml')
end

def logger
  @logger ||= Logger.new($stdout.tty? ? STDOUT : conf[:log]).tap do |l|
    l.progname = 'process_uploads'
    l.level = opts[:debug] ? Logger::DEBUG : Logger::INFO
  end
end

def pagerduty
  @pagerduty ||= Pagerduty.new conf[:pagerduty]
end

def notify(e)
  pagerduty.trigger(
    'SFTP Uploader',
    client: ENV['HOSTNAME'],
    details: "#{e.message}\n\n#{e.backtrace}"
  )

  Raven.capture_exception(e) if conf[:sentry_dsn]
end

def auth_query_string
  "auth=#{conf[:upload_auth_token]}&auth_provider=evertrueapptoken&app_key=#{conf[:upload_app_key]}"
end

def get_from_api(uri)
  uri = URI.parse(URI.encode(uri))

  http = Net::HTTP.new(uri.host, uri.port)
  http.use_ssl = true

  req = Net::HTTP::Get.new(uri.request_uri)
  response = http.request(req)
  fail "API error, Response: #{response.code}, body: #{response.body}" unless response.code.to_i == 200

  JSON.parse(response.body)
end

def send_to_api(uri, data, options = {})
  uri = URI.parse(URI.encode(uri))
  http = Net::HTTP.new(uri.host, uri.port)
  http.use_ssl = true

  logger.debug "Sending to API: #{uri}\nData: #{data.inspect}\nOptions: #{options.inspect}"

  req = (if options[:method] == 'put'
           Net::HTTP::Put.new(uri.request_uri)
         else
           Net::HTTP::Post.new(uri.request_uri)
         end)

  unless data.empty?
    req['Content-Type'] = 'application/json'
    req.body = data.to_json
  end

  response = http.request(req)

  logger.debug "Received from API: #{response.inspect}"

  fail "Error posting to API, data: #{data.inspect}. Response: " \
    "#{response.code}, body: #{response.body}" unless [200, 400].include? response.code.to_i

  JSON.parse(response.body)
end

def send_to_s3(org_slug, path)
  s3 = AWS::S3.new(
    access_key_id: conf[:aws_access_key_id],
    secret_access_key: conf[:aws_secret_access_key]
  )
  bucket = s3.buckets['onboarding.evertrue.com']
  now = DateTime.now.strftime('%Q')
  s3_filename = "#{now}-#{File.basename(path)}"
  bucket.objects["#{org_slug}/data/#{s3_filename}"].write(Pathname.new(path))
  s3_filename
end

def get_oid(org_slug)
  get_from_api(conf[:api_url] + "/auth/organizations/slug/#{org_slug}?" + auth_query_string)['id']
rescue => e
  logger.fatal "Error sending org_slug: #{org_slug}"
  raise e
end

def get_header_change_status_from_importer(oid, s3_filename, compression, gift_import, interaction_import)
  data = { 's3_filename' => s3_filename, 'notify' => 1, 'compression' => compression }
  if gift_import
    data['type'] = 'TRANSACTIONAL_CSV'
  elsif interaction_import
    data['type'] = 'NOTES_CSV'
  end

  r = send_to_api(
    conf[:api_url] +
    "/importer/v1/jobs/mapping_check?oid=#{oid}&#{auth_query_string}", data
  )['mapping_exists']

  logger.debug "Header change status: #{r.inspect} (class: #{r.class})"

  r
rescue => e
  logger.fatal "Error sending oid: #{oid}, s3_filename: #{s3_filename}"
  raise e
end

def post_to_importer(oid, s3_filename, compression, full_import, gift_import, interaction_import)
  data = { 's3_filename' => s3_filename, 'notify' => 1, 'compression' => compression }

  if gift_import
    data['type'] = 'TRANSACTIONAL_CSV'
  elsif interaction_import
    data['type'] = 'NOTES_CSV'
  end

  if full_import
    data['prune'] = full_import
  end

  send_to_api(conf[:api_url] + "/importer/v1/jobs?oid=#{oid}&#{auth_query_string}", data)['id']
rescue => e
  logger.fatal "Error sending oid: #{oid}, s3_filename: #{s3_filename}"
  raise e
end

def compress_if_not_already(path, compression)
  return path if compression != 'NONE'

  logger.debug "Compressing #{path}"
  Zlib::GzipWriter.open("#{path}.gz", Zlib::BEST_COMPRESSION) do |gz|
    gz.mtime = File.mtime(path)
    gz.orig_name = File.basename(path)
    File.open(path) do |f|
      IO.copy_stream(f, gz)
    end
  end

  logger.debug("Removing original file: #{path}")
  FileUtils.rm(path)
  f = "#{path}.gz"
  logger.debug("Compressed file: #{f}")
  f
end

def no_auto_ingest?(oid)
  auto_ingest_dna_value = get_from_api(
    conf[:api_url] +
    "/dna/setting_values/ET.Importer.IngestionMode?oid=#{oid}&#{auth_query_string}"
  )['settings']['ET.Importer.IngestionMode']['value']
  logger.debug "auto_ingest_dna_value: #{auto_ingest_dna_value}"
  auto_ingest_dna_value.nil? || (auto_ingest_dna_value != 'AutoIngest')
end

def process(org_slug, path, compression)
  uploaded = false

  # Ensure that a file is not currently being uploaded
  if system("lsof #{Shellwords.shellescape path}")
    logger.warn "Skipped processing #{path} because it is in use"
    return uploaded
  end

  full_import = !(File.basename(path) =~ /\.full\./i).nil?
  logger.debug "Full import? #{full_import}"

  gift_import = !(File.basename(path) =~ /\.gifts\./i).nil?
  logger.debug "Gift import? #{gift_import}"

  interaction_import = !(File.basename(path) =~ /\.interactions\./i).nil?
  logger.debug "Interactions import? #{interaction_import}"

  oid = get_oid(org_slug)
  logger.debug "Got OID: #{oid}"

  s3_filename = send_to_s3(org_slug, path)
  logger.debug "S3 Filename: #{s3_filename}"

  logger.debug 'Getting header change status from importer'
  mapping_exists = get_header_change_status_from_importer(oid, s3_filename, compression, gift_import, interaction_import)
  job_id = post_to_importer(oid, s3_filename, compression, full_import, gift_import, interaction_import)
  logger.debug "Importer JOB ID: #{job_id}"

  if mapping_exists
    logger.debug 'Mapping exists'
    if no_auto_ingest?(oid)
      logger.info "Skipping auto-ingestion for #{org_slug}"
    else
      # Queue the job with the importer
      send_to_api(
        conf[:api_url] +
        "/importer/v1/jobs/queue/#{job_id}?oid=#{oid}&#{auth_query_string}", {}
      )

      logger.info "Queued file #{path} for processing"
      uploaded = true
    end
  else
    logger.warn 'Mapping does not exist'
    data = { 'value' => 'NotifyOnly' }

    logger.debug 'Changing DNA value to NotifyOnly'

    send_to_api(
      conf[:api_url] +
      "/dna/setting_values/ET.Importer.IngestionMode?oid=#{oid}&#{auth_query_string}",
      data,
      method: 'put'
    )

    msg = "CSV headers not recognized in file #{path} for #{org_slug} (Job ID: #{job_id})"
    logger.warn msg
    email_notify msg
  end

  compressed_path = compress_if_not_already(path, compression)

  logger.debug 'Changing compressed file ownership to 0700'
  FileUtils.chmod(0700, compressed_path)

  logger.debug 'Changing compressed file ownership to root:root'
  FileUtils.chown('root', 'root', compressed_path)

  logger.debug "Moving compressed file to #{conf[:archive_dir]}"
  FileUtils.mv(compressed_path, conf[:archive_dir])

  uploaded
end

def email_notify(msg)
  support_email = opts[:debug] ? ENV['DEBUG_EMAIL'] : conf[:support_email]
  onboarding_email = opts[:debug] ? ENV['DEBUG_EMAIL'] : conf[:onboarding_email]

  Pony.mail(
    to: support_email + "," + onboarding_email,
    from: 'sftp-uploader@priv.evertrue.com',
    subject: 'SFTP Uploader Alert',
    body: msg
  )
end

def main
  fail 'DEBUG_EMAIL required in debug mode' if opts[:debug] && !ENV['DEBUG_EMAIL']

  if conf[:sentry_dsn]
    Raven.configure do |config|
      config.dsn = conf[:sentry_dsn]
      config.logger = logger
    end
  end

  processed_users = conf[:unames].each_with_object([]) do |uname, processed_usernames|
    logger.debug "Processing user #{uname}"

    if uname == 'trial0928'
      logger.debug 'Skipping trial user trial0928'
      next
    end

    org_slug = uname.sub(/\d{4,}$/, '')

    logger.debug "Using org slug #{org_slug}"

    begin
      logger.debug 'Searching for files to upload'
      Dir.glob("#{conf[:upload_dir]}/#{uname}/uploads/*") do |path|
        logger.debug "- #{path}"
        begin
          compression_type = (
            case path
            when /.*\.csv$/i
              'NONE'
            when /.*\.gz$/i
              'GZIP'
            when /.*\.zip$/i
              'ZIP'
            else
              logger.debug "Skipping unrecognized file #{path}"
              next
            end
          )

          logger.debug "Compression type: #{compression_type}"

          if process(org_slug, path, compression_type)
            logger.debug "File #{path} was processed successfully"
            processed_usernames << uname
          end
        rescue => e
          logger.fatal "Error processing path #{path}: #{e}"
          logger.fatal e.message
          logger.fatal e.backtrace.join("\n")
          notify e
        end
      end
    rescue => e
      logger.fatal "Error processing org #{org_slug}: #{e}"
      logger.fatal e.message
      logger.fatal e.backtrace.join("\n")
      notify e
    end
  end

  logger.info "Uploaded data from: #{processed_users.join(', ')}" unless processed_users.empty?
rescue Interrupt => e
  logger.info "Received #{e.class}"
  exit 99
rescue SignalException => e
  logger.info "Received: #{e.signm} (#{e.signo})"
  exit 2
rescue SystemExit => e
  exit e.status
rescue Exception => e # Need to rescue "Exception" so that Sentry/PD gets it
  notify e
  logger.fatal e.message
  logger.fatal e.backtrace.join("\n")
  raise e
end

main
