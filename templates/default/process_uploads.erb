#!/usr/bin/env ruby

# Generated by Chef for <%= node['fqdn'] %>
# Local modifications will be overwritten.

require 'aws-sdk'
require 'find'
require 'fileutils'
require 'net/http'
require 'net/http/post/multipart'
require 'uri'
require 'json'
require 'zlib'
require 'zip'

def get_dna(org_slug, key)
  uri = URI.parse(URI.encode("https://api.evertrue.com/1.0/#{org_slug}/dna/#{key}"))
  http = Net::HTTP.new(uri.host, uri.port)
  http.use_ssl = true

  req = Net::HTTP::Get.new(uri.path)
  response = http.request(req)

  body = JSON.parse(response.body)
  body["response"]["data"]
end

def send_to_new_importer(org_slug, path, compression, is_full_import)
  # send to s3
  # post to new importer endpoint
end

def send_to_legacy_importer(org_slug, path, compression, is_full_import)
  case compression
    when "NONE"
      File.open(path) do |f|
        send_decompressed_stream_to_legacy_importer(org_slug, path, is_full_import)
      end
    when "GZIP"
      Zlib::GzipReader.open(path) do |gz|
        extractPath = "#{File.dirname(path)}/#{gz.orig_name}"
        File.open(extractPath, "w+") do |f|
          IO.copy_stream(gz, f)
        end
        send_decompressed_stream_to_legacy_importer(org_slug, extractPath, is_full_import)
        FileUtils.rm(extractPath)
      end
    when "ZIP"
      Zip::File.open(path) do |zf|
        zf.each do |f|
          if f.name =~ /.*\.csv$/i
            extractPath = "#{File.dirname(path)}/#{f.name}"
            f.extract(extractPath)
            send_decompressed_stream_to_legacy_importer(org_slug, extractPath, is_full_import)
            FileUtils.rm(extractPath)
          end
        end
      end
  end
end

def send_decompressed_stream_to_legacy_importer(org_slug, path, is_full_import)
  uri = URI.parse(URI.encode("https://api.evertrue.com/1.0/#{org_slug}/importjob"))
  http = Net::HTTP.new(uri.host, uri.port)
  http.use_ssl = true

  req = Net::HTTP::Post::Multipart.new(uri.path, "type" => "file", "prune" => (is_full_import ? 1 : 0), "file" => UploadIO.new(path, "text/csv"))

  response = http.request(req)
  raise "Error sending #{path}. Response: #{response.code}, body: #{response.body}" unless response.code.to_i == 200
end

def compress_if_not_already(path, compression)
  if compression != "NONE"
    return path
  end

  Zlib::GzipWriter.open("#{path}.gz", Zlib::BEST_COMPRESSION) do |gz|
    gz.mtime = File.mtime(path)
    gz.orig_name = File.basename(path)
    File.open(path) do |f|
      IO.copy_stream(f, gz)
    end
  end

  FileUtils.rm(path)
  "#{path}.gz"
end

def process(org_slug, path, compression)
  begin
    return if !`lsof #{path}`.empty?

    is_full_import = !(File.basename(path) =~ /\.full\./i).nil?

    to_new_importer = !get_dna(org_slug, "ET.Routing.Upload.Importer2").to_i.zero?

    if to_new_importer
      send_to_new_importer(org_slug, path, compression, is_full_import)
    else
      send_to_legacy_importer(org_slug, path, compression, is_full_import)
    end

    puts "sent file #{path} for processing"

    compressed_path = compress_if_not_already(path, compression)

    FileUtils.chmod(0700, compressed_path)
    FileUtils.chown('root', 'root', compressed_path)
    FileUtils.mv(compressed_path, '/var/evertrue/uploads')

  rescue Exception => e
    puts e
  end
end

unames = [<% Array(@unames).each do |uname| %>"<%= uname %>", <% end -%>]

unames.each do |uname|
  org_slug = /(.*?)\d+$/.match(uname)[1]

  auto_ingest = get_dna(org_slug, "ET.Importer.IngestionMode")
  if auto_ingest.nil? || auto_ingest != "AutoIngest"
    puts "skipped auto-ingestion for #{org_slug}"
    next
  end

  Find.find("/home/#{uname}/uploads") do |path|
    case path
    when /.*\.csv$/i
      process(org_slug, path, "NONE")
    when /.*\.gz$/i
      process(org_slug, path, "GZIP")
    when /.*\.zip$/i
      process(org_slug, path, "ZIP")
    end
  end
end
